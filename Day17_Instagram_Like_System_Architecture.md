# Instagram's 7-Layer Architecture: How 1 Million Likes Don't Break the Internet
### Day 17 of 50 - System Design Interview Preparation Series

**By Sunchit Dudeja**

---

## ğŸ¯ Welcome to Day 17!

Yesterday, we explored Redis Sorted Sets for real-time leaderboards. Today, we dive into one of the most fascinating system design challenges â€” **how Instagram handles millions of likes on viral posts without crashing**.

> When Hardik Pandya posts a photo after winning the World Cup, 1 million likes hit Instagram in 6 minutes. How does the system survive?

---

## ğŸ† THE CHALLENGE: HARDIK'S VIRAL POST

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    THE SCENARIO                                  â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                  â”‚
â”‚   ğŸ“¸ Hardik Pandya posts World Cup victory photo                â”‚
â”‚                                                                  â”‚
â”‚   Within 6 minutes:                                              â”‚
â”‚   â€¢ 1,000,000 likes                                              â”‚
â”‚   â€¢ 2,778 likes per SECOND                                       â”‚
â”‚   â€¢ Peak burst: 10,000 likes/second                             â”‚
â”‚                                                                  â”‚
â”‚   Challenges:                                                    â”‚
â”‚   â”œâ”€â”€ Handle 10K writes/second without crashing                 â”‚
â”‚   â”œâ”€â”€ Show updated count to 50M concurrent viewers              â”‚
â”‚   â”œâ”€â”€ Never lose a single like                                  â”‚
â”‚   â”œâ”€â”€ Keep response time under 100ms                            â”‚
â”‚   â””â”€â”€ Do this while running 1000s of other operations           â”‚
â”‚                                                                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**The naive approach would crash instantly:**
```sql
-- 10,000 times per second:
UPDATE posts SET like_count = like_count + 1 WHERE id = 'hardik_post';
-- Database row lock contention = DEATH
```

Let's see how Instagram actually handles this.

---

## ğŸ—ï¸ INSTAGRAM'S 7-LAYER ARCHITECTURE

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    THE 7 LAYERS                                  â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                  â”‚
â”‚   Layer 1: Client-Side Batching & Debouncing                    â”‚
â”‚      â†“                                                           â”‚
â”‚   Layer 2: Edge Server Accumulation                              â”‚
â”‚      â†“                                                           â”‚
â”‚   Layer 3: Kafka Event Streaming                                 â”‚
â”‚      â†“                                                           â”‚
â”‚   Layer 4: Async Processing Workflows                            â”‚
â”‚      â†“                                                           â”‚
â”‚   Layer 5: Counter Cache System                                  â”‚
â”‚      â†“                                                           â”‚
â”‚   Layer 6: Database Write Strategy                               â”‚
â”‚      â†“                                                           â”‚
â”‚   Layer 7: Read Path Optimization                                â”‚
â”‚                                                                  â”‚
â”‚   Result: 1M likes â†’ ~100 database writes                       â”‚
â”‚                                                                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ“± LAYER 1: CLIENT-SIDE BATCHING & DEBOUNCING

### The First Line of Defense: Your Phone

Before a like even leaves your device, Instagram's app does clever optimization:

```swift
// Instagram App Code (Simplified)
class LikeManager {
    private var likeQueue = [String: LikeOperation]()  // PostID -> Operation
    private let debounceDelay = 2.0  // Wait 2 seconds
    
    func userTappedLike(postId: String) {
        // 1. Immediate UI feedback (optimistic update)
        updateUIInstantly(postId: postId)
        
        // 2. Debounce - Wait for more taps
        cancelPendingLike(postId: postId)
        scheduleLike(postId: postId, delay: debounceDelay)
    }
    
    private func scheduleLike(postId: String, delay: TimeInterval) {
        DispatchQueue.main.asyncAfter(deadline: .now() + delay) {
            // Batch ALL pending likes for this user
            let batch = self.collectPendingLikes()
            
            // Send batch to server
            sendBatchToServer(batch: batch)
        }
    }
}
```

### What This Achieves

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    CLIENT-SIDE REDUCTION                         â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                  â”‚
â”‚   User scrolls through feed, liking 10 posts quickly:           â”‚
â”‚                                                                  â”‚
â”‚   WITHOUT debouncing:                                            â”‚
â”‚   Like 1 â†’ Request 1                                             â”‚
â”‚   Like 2 â†’ Request 2                                             â”‚
â”‚   ...                                                            â”‚
â”‚   Like 10 â†’ Request 10                                           â”‚
â”‚   Total: 10 network requests                                     â”‚
â”‚                                                                  â”‚
â”‚   WITH debouncing (2 second wait):                               â”‚
â”‚   Like 1 â†’ Wait...                                               â”‚
â”‚   Like 2 â†’ Reset timer, wait...                                  â”‚
â”‚   ...                                                            â”‚
â”‚   Like 10 â†’ Reset timer, wait...                                 â”‚
â”‚   [2 seconds pass]                                               â”‚
â”‚   â†’ Send 1 request with all 10 likes!                           â”‚
â”‚   Total: 1 network request                                       â”‚
â”‚                                                                  â”‚
â”‚   âš¡ Result: 10x reduction in network requests!                  â”‚
â”‚                                                                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Optimistic UI Update

```
The user sees the heart turn red INSTANTLY, even though
the request hasn't been sent yet.

If the server later rejects the like:
- App silently unlikes
- User rarely notices

This makes Instagram feel FAST even when the network is slow.
```

---

## ğŸŒ LAYER 2: EDGE SERVER ACCUMULATION

### Regional Edge Servers Batch Further

Likes from India don't go directly to Instagram's main servers in the USA. They first hit a regional edge server in Mumbai:

```python
# Instagram Edge Server (Mumbai)
class LikeAccumulator:
    def __init__(self):
        self.counters = defaultdict(int)  # post_id -> accumulated_likes
        self.batch_size = 1000
        self.flush_interval = 5  # seconds
    
    def receive_like(self, post_id, user_id):
        # 1. Increment in-memory counter
        self.counters[post_id] += 1
        
        # 2. Store like in local Redis for deduplication
        redis.sadd(f"post_likes:{post_id}", user_id)
        
        # 3. Check if should flush
        if (self.counters[post_id] >= self.batch_size or 
            time.time() - last_flush > self.flush_interval):
            self.flush_to_central(post_id)
    
    def flush_to_central(self, post_id):
        count = self.counters.pop(post_id, 0)
        if count > 0:
            # Send batch update to central
            kafka.send("like_updates", {
                "post_id": post_id,
                "increment": count,
                "region": "mumbai",
                "timestamp": time.time()
            })
```

### Edge Server Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    EDGE SERVER BATCHING                          â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                  â”‚
â”‚   Mumbai Edge Server:                                            â”‚
â”‚   â”œâ”€â”€ Receives: 5,000 likes/second from India                   â”‚
â”‚   â”œâ”€â”€ Batches: Every 1,000 likes OR 5 seconds                   â”‚
â”‚   â””â”€â”€ Sends: 5 batch updates/second to central                  â”‚
â”‚                                                                  â”‚
â”‚   Virginia Edge Server:                                          â”‚
â”‚   â”œâ”€â”€ Receives: 2,000 likes/second from USA East                â”‚
â”‚   â”œâ”€â”€ Batches: Every 1,000 likes OR 5 seconds                   â”‚
â”‚   â””â”€â”€ Sends: 2 batch updates/second to central                  â”‚
â”‚                                                                  â”‚
â”‚   Singapore Edge Server:                                         â”‚
â”‚   â”œâ”€â”€ Receives: 3,000 likes/second from SE Asia                 â”‚
â”‚   â”œâ”€â”€ Batches: Every 1,000 likes OR 5 seconds                   â”‚
â”‚   â””â”€â”€ Sends: 3 batch updates/second to central                  â”‚
â”‚                                                                  â”‚
â”‚   âš¡ Total reduction: 10,000 likes/sec â†’ 10 batches/sec         â”‚
â”‚                                                                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ“¨ LAYER 3: KAFKA EVENT STREAMING

### The Like Pipeline

Kafka acts as a buffer between the high-velocity like events and the slower database writes:

```java
// Instagram's Kafka Topics Architecture
Topics:
1. likes-raw            â†’ Raw like events (deduplicated)
2. likes-aggregated     â†’ Per-post aggregates (5-second windows)
3. likes-enriched       â†’ With user/profile data
4. likes-persisted      â†’ Ready for database write

// Each topic has 1000+ partitions for horizontal scaling
// Hardik's post gets dedicated partitions to prevent fan-out

// Consumer Group for Hardik's viral post
@KafkaListener(
    topics = "likes-raw",
    groupId = "hardik-post-123",
    concurrency = "50"  // 50 consumers for this post alone!
)
public void processViralLike(LikeEvent event) {
    // Special handling for viral content
    if (event.isViral()) {
        viralLikeProcessor.process(event);
    } else {
        regularLikeProcessor.process(event);
    }
}
```

### Kafka Architecture for Likes

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    KAFKA LIKE PIPELINE                           â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                  â”‚
â”‚   Edge Servers                                                   â”‚
â”‚       â†“                                                          â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚   â”‚               KAFKA CLUSTER                              â”‚   â”‚
â”‚   â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”           â”‚   â”‚
â”‚   â”‚  â”‚ Partition â”‚  â”‚ Partition â”‚  â”‚ Partition â”‚  ...      â”‚   â”‚
â”‚   â”‚  â”‚     0     â”‚  â”‚     1     â”‚  â”‚     2     â”‚           â”‚   â”‚
â”‚   â”‚  â”‚ (Regular) â”‚  â”‚ (Regular) â”‚  â”‚ (VIRAL!)  â”‚ â† Hardik  â”‚   â”‚
â”‚   â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜           â”‚   â”‚
â”‚   â”‚       â†“              â†“              â†“                    â”‚   â”‚
â”‚   â”‚    2 workers      2 workers     50 workers              â”‚   â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚       â†“                                                          â”‚
â”‚   Consumer Groups (Process in parallel)                          â”‚
â”‚       â†“                                                          â”‚
â”‚   Counter Cache + Database                                       â”‚
â”‚                                                                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Why Kafka?

| Benefit | How It Helps |
|---------|--------------|
| **Buffering** | Absorbs burst traffic, processes at sustainable rate |
| **Partitioning** | Viral posts get dedicated partitions |
| **Replayability** | If consumer crashes, replay from offset |
| **Ordering** | Within partition, likes are processed in order |
| **Horizontal Scaling** | Add more partitions/consumers as needed |

---

## âš™ï¸ LAYER 4: ASYNC PROCESSING WORKFLOWS

### Celery-based Worker Pipeline

```python
# Instagram's Celery Task Flow
@app.task(queue='likes_priority', rate_limit='1000/s')
def process_like_batch(post_id, user_ids):
    # Step 1: Deduplication (Bloom Filter + Redis)
    unique_likes = deduplicate_likes(post_id, user_ids)
    
    # Step 2: Update counter cache (Redis)
    redis.incrby(f"post:likes:{post_id}", len(unique_likes))
    
    # Step 3: Update timeline rankings
    update_post_ranking(post_id, len(unique_likes))
    
    # Step 4: Async notification fanout
    if should_notify_owner(post_id):
        fanout_notifications.delay(post_id, len(unique_likes))
    
    # Step 5: Batch database write (later!)
    enqueue_for_persistence(post_id, unique_likes)


# Viral post gets dedicated queue with higher rate limit
@app.task(queue='viral_likes', rate_limit='10000/s')
def process_viral_like_batch(post_id, user_ids):
    # Optimized pipeline for viral content
    # Uses different algorithms, storage, etc.
    pass
```

### The Processing Pipeline

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    ASYNC PROCESSING STEPS                        â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                  â”‚
â”‚   Step 1: DEDUPLICATION                                          â”‚
â”‚   â”œâ”€â”€ Bloom Filter: Quick "probably not duplicate" check        â”‚
â”‚   â”œâ”€â”€ Redis SET: Definitive duplicate check                     â”‚
â”‚   â””â”€â”€ Result: Remove likes from same user                       â”‚
â”‚                                                                  â”‚
â”‚   Step 2: COUNTER UPDATE                                         â”‚
â”‚   â”œâ”€â”€ Redis INCRBY: Atomic increment                            â”‚
â”‚   â”œâ”€â”€ Update display cache                                       â”‚
â”‚   â””â”€â”€ Result: Instant count update for viewers                  â”‚
â”‚                                                                  â”‚
â”‚   Step 3: RANKING UPDATE                                         â”‚
â”‚   â”œâ”€â”€ Update post score in timeline algorithm                   â”‚
â”‚   â”œâ”€â”€ Bump post in "trending" calculations                      â”‚
â”‚   â””â”€â”€ Result: Post rises in feeds                               â”‚
â”‚                                                                  â”‚
â”‚   Step 4: NOTIFICATIONS                                          â”‚
â”‚   â”œâ”€â”€ Check notification preferences                             â”‚
â”‚   â”œâ”€â”€ Aggregate: "Hardik, you got 100K likes!"                  â”‚
â”‚   â””â”€â”€ Result: Owner notified (batched, not per like)            â”‚
â”‚                                                                  â”‚
â”‚   Step 5: PERSISTENCE                                            â”‚
â”‚   â”œâ”€â”€ Enqueue for database write                                â”‚
â”‚   â”œâ”€â”€ Batch with other likes                                    â”‚
â”‚   â””â”€â”€ Result: Eventual database consistency                     â”‚
â”‚                                                                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ’¾ LAYER 5: COUNTER CACHE SYSTEM

### Three-Tier Counter Storage

```python
class LikeCounterSystem:
    def __init__(self):
        # L1: Local cache (per server)
        self.local_cache = LRUCache(maxsize=10000)
        
        # L2: Regional Redis cluster
        self.redis = RedisCluster()
        
        # L3: Global counter service (Cassandra)
        self.counter_service = CounterService()
    
    def increment(self, post_id, delta=1):
        # 1. Update local cache (atomic)
        self.local_cache.incr(post_id, delta)
        
        # 2. Async flush to Redis every second
        if time.time() - last_flush > 1:
            self.flush_local_to_redis()
        
        # 3. Background sync to global counter
        self.async_sync_to_global(post_id)
    
    def get_count(self, post_id):
        # Read-through cache
        count = self.local_cache.get(post_id)
        if count is None:
            count = self.redis.get(f"post:likes:{post_id}")
            if count is None:
                count = self.counter_service.get(post_id)
                # Populate caches
                self.redis.setex(f"post:likes:{post_id}", 3600, count)
            self.local_cache.set(post_id, count)
        return count
```

### Counter Cache Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    THREE-TIER COUNTER CACHE                      â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                  â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                           â”‚
â”‚   â”‚   L1: LOCAL     â”‚  â† Per-server in-memory cache             â”‚
â”‚   â”‚   (LRU Cache)   â”‚  â† 10K hot posts                          â”‚
â”‚   â”‚   Latency: 1Î¼s  â”‚  â† Flush to L2 every 1 second             â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜                                           â”‚
â”‚            â”‚ miss                                                â”‚
â”‚            â†“                                                     â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                           â”‚
â”‚   â”‚   L2: REDIS     â”‚  â† Regional Redis cluster                 â”‚
â”‚   â”‚   (Cluster)     â”‚  â† All active posts                       â”‚
â”‚   â”‚   Latency: 1ms  â”‚  â† Sync to L3 every 5 seconds             â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜                                           â”‚
â”‚            â”‚ miss                                                â”‚
â”‚            â†“                                                     â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                           â”‚
â”‚   â”‚   L3: CASSANDRA â”‚  â† Global counter service                 â”‚
â”‚   â”‚   (Persistent)  â”‚  â† All posts ever                         â”‚
â”‚   â”‚   Latency: 10ms â”‚  â† Source of truth                        â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                                           â”‚
â”‚                                                                  â”‚
â”‚   READ: L1 â†’ L2 â†’ L3 (with cache population)                    â”‚
â”‚   WRITE: L1 â†’ async L2 â†’ async L3                               â”‚
â”‚                                                                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ—„ï¸ LAYER 6: DATABASE WRITE STRATEGY

### Eventually Consistent Database Updates

```python
# Instagram's Database Writer
class LikePersistenceEngine:
    def __init__(self):
        self.write_queue = deque()
        self.batch_size = 10000
        self.max_delay = 60  # Maximum 60 seconds delay
    
    def enqueue_likes(self, post_id, user_ids):
        # Add to write queue
        self.write_queue.append((post_id, user_ids))
        
        # Check if should flush
        if (len(self.write_queue) >= self.batch_size or
            time.time() - last_write > self.max_delay):
            
            # Batch write to database
            self.flush_to_database()
    
    def flush_to_database(self):
        batch = self.collect_batch()
        
        # Use UPSERT for idempotency
        query = """
        INSERT INTO likes (post_id, user_id, created_at)
        VALUES %s
        ON CONFLICT (post_id, user_id) DO NOTHING
        """
        
        # Update post counter with single query
        counter_update = """
        UPDATE posts 
        SET like_count = like_count + $1
        WHERE id = $2
        """
        
        # Execute in transaction
        with transaction.atomic():
            execute_batch(query, batch)
            execute(counter_update, [total_count, post_id])
```

### Database Write Optimization

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    DATABASE WRITE STRATEGY                       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                  â”‚
â”‚   NAIVE APPROACH (would crash):                                  â”‚
â”‚   1M likes â†’ 1M INSERT statements â†’ 1M row locks â†’ DEATH        â”‚
â”‚                                                                  â”‚
â”‚   INSTAGRAM'S APPROACH:                                          â”‚
â”‚   1M likes â†’ Queue in memory                                     â”‚
â”‚           â†’ Batch every 10K or 60 seconds                       â”‚
â”‚           â†’ 100 batch writes                                     â”‚
â”‚           â†’ 100 counter updates                                  â”‚
â”‚                                                                  â”‚
â”‚   âš¡ 1,000,000 likes â†’ 100 database transactions                â”‚
â”‚   âš¡ 10,000x reduction in database load!                        â”‚
â”‚                                                                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ‘ï¸ LAYER 7: READ PATH OPTIMIZATION

### The Like Count Display Strategy

```python
def get_like_display_count(post_id):
    cached_count = redis.get(f"post:likes:display:{post_id}")
    
    if cached_count:
        return format_count(cached_count)
    
    # Real count with approximations for viral posts
    actual_count = counter_service.get(post_id)
    
    # Instagram's rounding strategy:
    if actual_count < 1000:
        display = str(actual_count)  # "999"
    elif actual_count < 1000000:
        # Round to nearest K: 123456 â†’ "123K"
        display = f"{round(actual_count / 1000)}K"
    else:
        # Round to nearest M: 1234567 â†’ "1.2M"
        display = f"{round(actual_count / 1000000, 1)}M"
    
    # Cache for 30 seconds
    redis.setex(f"post:likes:display:{post_id}", 30, display)
    
    return display
```

### Why Approximate Counts?

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    WHY "1.2M" NOT "1,234,567"?                   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                  â”‚
â”‚   1. REDUCES COUNTER SERVICE LOAD                               â”‚
â”‚      Fewer cache invalidations when count changes               â”‚
â”‚      "1.2M" stays valid for 100K more likes!                    â”‚
â”‚                                                                  â”‚
â”‚   2. USERS DON'T NEED EXACT COUNT                               â”‚
â”‚      Nobody cares if it's 1,234,567 vs 1,234,890                â”‚
â”‚      "1.2M" communicates the same meaning                       â”‚
â”‚                                                                  â”‚
â”‚   3. PREVENTS "LIKE COUNT WARS"                                 â”‚
â”‚      Exact counts encourage unhealthy competition               â”‚
â”‚      Approximate counts reduce anxiety                          â”‚
â”‚                                                                  â”‚
â”‚   4. SAVES BANDWIDTH                                             â”‚
â”‚      "1.2M" = 4 bytes                                           â”‚
â”‚      "1,234,567" = 9 bytes                                      â”‚
â”‚      At billions of requests = petabytes saved                  â”‚
â”‚                                                                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## âš¡ VIRAL CONTENT SPECIAL HANDLING

### Hardik's Post Gets Special Treatment

```yaml
Special Treatment for Viral Posts:
  - Dedicated Kafka partitions: Partition 7 (just for this post)
  - Priority queue: 'viral_likes' with 1000 workers
  - Hot cache: Pinned in Redis (never evicted)
  - Database shard: Pre-warmed connections
  - Monitoring: Special dashboard with 1-second granularity
  - Fallback: Circuit breakers to prevent cascade failure
```

### Circuit Breaker for Viral Posts

```python
@circuit_breaker(
    failure_threshold=5,
    recovery_timeout=30,
    expected_exception=RateLimitException
)
def process_viral_like(post_id, user_id):
    # Try normal path first
    try:
        return normal_like_flow(post_id, user_id)
    except RateLimitException:
        # Degrade gracefully
        return fallback_viral_flow(post_id, user_id)


def fallback_viral_flow(post_id, user_id):
    # 1. Write to local SSD temporarily
    write_to_local_log(post_id, user_id)
    
    # 2. Update counter cache only
    redis.incr(f"post:likes:{post_id}")
    
    # 3. Return success to user
    return {"status": "queued", "count": get_cached_count(post_id)}
    
    # Background job will reconcile later
```

---

## ğŸŒ THE LIKE COUNT DILEMMA: DOES EVERYONE SEE THE SAME NUMBER?

### The Reality: NO!

**Instagram's Secret:** Your like count is personalized, approximate, and eventually consistent â€” and that's by design.

### Regional Variations

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚            DIFFERENT DATA CENTERS, DIFFERENT COUNTS              â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                  â”‚
â”‚   Hardik's Post: Actual Count = 1,234,567                       â”‚
â”‚                                                                  â”‚
â”‚   Mumbai Data Center (India):                                    â”‚
â”‚   â”œâ”€â”€ Sees: 1,200,000                                           â”‚
â”‚   â”œâ”€â”€ Updates: Every 5 seconds                                  â”‚
â”‚   â””â”€â”€ Real-time for Indian users                                â”‚
â”‚                                                                  â”‚
â”‚   Virginia Data Center (USA):                                    â”‚
â”‚   â”œâ”€â”€ Sees: 1,100,000                                           â”‚
â”‚   â”œâ”€â”€ Updates: Every 10 seconds                                 â”‚
â”‚   â””â”€â”€ 15-30 second delay for US users                           â”‚
â”‚                                                                  â”‚
â”‚   Singapore Data Center (SE Asia):                               â”‚
â”‚   â”œâ”€â”€ Sees: 1,150,000                                           â”‚
â”‚   â”œâ”€â”€ Updates: Every 8 seconds                                  â”‚
â”‚   â””â”€â”€ Mix of regional updates                                   â”‚
â”‚                                                                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Personalized Filtering

```python
def personalize_like_count(viewer, post_id, raw_count):
    """
    Personalizes count based on YOUR context
    """
    personalized = raw_count
    
    # 1. Remove likes from people YOU blocked
    blocked_users = get_blocked_users(viewer.id)
    if blocked_users:
        blocked_likes = count_likes_from_users(post_id, blocked_users)
        personalized -= blocked_likes
    
    # 2. Remove likes from suspected bots/spammers
    if viewer.prefs.get("hide_spam_likes", True):
        spam_likes = estimate_spam_likes(post_id)
        personalized -= spam_likes
    
    # 3. Remove likes from muted accounts
    muted_accounts = get_muted_accounts(viewer.id)
    if muted_accounts:
        muted_likes = count_likes_from_accounts(post_id, muted_accounts)
        personalized -= muted_likes
    
    return max(0, personalized)  # Never show negative
```

### Four Users See FOUR Different Counts

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚            SAME POST, DIFFERENT COUNTS                           â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                  â”‚
â”‚   User 1: Hardik's Teammate in Mumbai on 5G                     â”‚
â”‚   Sees: "1.2M" (accurate, real-time, rounded)                   â”‚
â”‚   Why: Mumbai DC, 5G, celeb tier, no filters                    â”‚
â”‚                                                                  â”‚
â”‚   User 2: Fan in Rural India on 3G                              â”‚
â”‚   Sees: "1M" (stale, approximate)                               â”‚
â”‚   Why: Cached from 5 min ago, 3G delay, tier 3 freshness        â”‚
â”‚                                                                  â”‚
â”‚   User 3: User in USA Who Blocked Spam Accounts                 â”‚
â”‚   Sees: "1.1M" (filtered, delayed)                              â”‚
â”‚   Why: Virginia DC, removed 100K spam likes, 15-sec delay       â”‚
â”‚                                                                  â”‚
â”‚   User 4: Instagram Employee in A/B Test                        â”‚
â”‚   Sees: "1,234,567" (exact, debug mode)                         â”‚
â”‚   Why: Internal tooling, exact counting, real-time              â”‚
â”‚                                                                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ“Š MONITORING & AUTOSCALING

### Instagram's Monitoring Stack

```python
# Key metrics monitored
METRICS = {
    "likes_per_second": "rate(likes_received[1m])",
    "processing_latency_p99": "histogram_quantile(0.99, like_processing_duration)",
    "queue_backlog": "kafka_topic_messages{topic='likes-raw'}",
    "cache_hit_rate": "redis_cache_hits / redis_cache_requests",
    "database_connection_pool": "database_connections_active",
}

# Auto-scaling triggers
AUTOSCALE_RULES = {
    "likes_per_second > 5000": "scale_like_processors(2x)",
    "processing_latency_p99 > 1000ms": "scale_like_processors(3x)",
    "queue_backlog > 100000": "add_dedicated_viral_processor()",
    "cache_hit_rate < 0.95": "increase_redis_cache_size()",
}

# For Hardik's post specifically:
SPECIAL_RULES = {
    "post_123_likes_per_second > 10000": "allocate_dedicated_resources()",
    "post_123_engagement > 500000": "enable_approximate_counting()",
}
```

---

## ğŸ”§ HANDLING FAILURES

### Partial Failure Strategy

```python
def like_post_safe(post_id, user_id):
    try:
        # Attempt 1: Primary flow
        return like_post_primary(post_id, user_id)
    except Exception as e:
        logger.warning(f"Primary flow failed: {e}")
        
        try:
            # Attempt 2: Write-ahead log
            write_to_wal(post_id, user_id)
            return {"status": "accepted", "mode": "degraded"}
        except Exception as e2:
            logger.error(f"WAL also failed: {e2}")
            
            # Final fallback: Client-side retry
            return {
                "status": "retry_later",
                "retry_after": 30,
                "client_cache_key": f"pending_like_{post_id}"
            }

# Client handles retry:
# localStorage.setItem(`pending_like_${postId}`, Date.now());
# Retry after 30 seconds automatically
```

---

## ğŸ¯ KEY STRATEGIES SUMMARIZED

| Strategy | What It Does | Reduction |
|----------|--------------|-----------|
| **Debouncing** | Wait for multiple likes before sending | 10x |
| **Batching** | Process 1000 likes as one operation | 1000x |
| **Async Processing** | Acknowledge immediately, process later | âˆ throughput |
| **Counter Caches** | Redis for reads, async DB sync | 1000x DB load |
| **Event Streaming** | Kafka buffers and load levels | Absorbs bursts |
| **Sharding** | Different posts on different infra | Isolation |
| **Approximation** | "1.2M" instead of "1,234,567" | 10x cache efficiency |
| **Circuit Breakers** | Fail fast, degrade gracefully | Prevents cascade |
| **Write-ahead Logs** | Never lose data, reconcile later | Durability |
| **Viral Detection** | Special handling for trending | Dedicated resources |

---

## ğŸ’¡ WHY INSTAGRAM DOESN'T CRACK

### The Complete Picture

```
1M likes over 6 minutes becomes:

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    THE TRANSFORMATION                            â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                  â”‚
â”‚   Layer 1 (Client):     1M likes â†’ 100K batched requests        â”‚
â”‚   Layer 2 (Edge):       100K requests â†’ 100 Kafka messages      â”‚
â”‚   Layer 3 (Kafka):      100 messages â†’ buffered stream          â”‚
â”‚   Layer 4 (Workers):    Stream â†’ processed at sustainable rate  â”‚
â”‚   Layer 5 (Cache):      Updates â†’ instant reads for viewers     â”‚
â”‚   Layer 6 (Database):   Stream â†’ 100 batch writes               â”‚
â”‚   Layer 7 (Display):    100 writes â†’ cached "1.2M" display      â”‚
â”‚                                                                  â”‚
â”‚   âš¡ 1,000,000 likes â†’ 100 database writes                      â”‚
â”‚   âš¡ Users see updated count within 2 seconds                   â”‚
â”‚   âš¡ System never breaks a sweat                                â”‚
â”‚                                                                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## â“ Interview Practice

### Question 1:
> "How would you design a system to handle 10,000 likes per second on a viral post?"

**Answer:**
> "I'd implement a 7-layer architecture: (1) Client-side debouncing to batch likes, (2) Edge servers to accumulate regionally, (3) Kafka for event streaming and buffering, (4) Async workers with dedicated queues for viral content, (5) Three-tier counter cache (localâ†’Redisâ†’Cassandra), (6) Batched database writes with eventual consistency, (7) Approximate counting for display ('1.2M' not '1,234,567'). This transforms 10K writes/sec into ~10 database transactions/sec while maintaining sub-second count updates for viewers."

### Question 2:
> "Why does Instagram show different like counts to different users?"

**Answer:**
> "It's intentional for performance and personalization: (1) Regional caching means Mumbai and Virginia have different stale counts, (2) Personalized filtering removes likes from accounts you've blocked, (3) Approximate rounding means '1.2M' stays valid longer than exact counts, (4) A/B testing different counting algorithms, (5) Network-based optimization gives stale counts to slow connections. This reduces counter service load 100x while users barely notice the difference."

### Question 3:
> "What happens if the database is down but likes are still coming in?"

**Answer:**
> "We use write-ahead logging. Likes are written to a local SSD log first, counter cache is updated for immediate reads, and the user sees success. A background reconciliation job replays the WAL to the database when it recovers. Circuit breakers prevent cascade failures, and the system degrades gracefully â€” likes might take 60 seconds to appear in the database but users never see an error."

---

## ğŸ”— Connecting to Previous Days

| Day | Concept | How It Connects |
|-----|---------|-----------------|
| Day 13 | Circuit Breaker | Protects viral post processing from cascade failures |
| Day 15 | Redis Single-Threaded | Counter cache uses Redis's speed |
| Day 16 | Redis Sorted Sets | Could be used for leaderboards of top likers |

---

## âœ… Day 17 Action Items

1. **Implement debouncing** â€” Add 2-second debounce to your like buttons
2. **Use Kafka for writes** â€” Buffer high-velocity writes through event streaming
3. **Add counter caching** â€” Never read counts from database directly
4. **Build circuit breakers** â€” Protect against viral content spikes
5. **Approximate where possible** â€” "1.2M" is better than "1,234,567"

---

## ğŸ’¡ Key Takeaways

| Insight | Why It Matters |
|---------|----------------|
| Debounce at client | First line of defense against burst traffic |
| Edge accumulation | Regional batching before central processing |
| Kafka buffering | Absorbs spikes, processes at sustainable rate |
| Counter caches | Reads never touch database |
| Eventual consistency | Accuracy can wait, user experience can't |
| Approximate counting | "1.2M" is good enough |
| Viral detection | Special content needs special handling |

---

## ğŸ¯ The Architect's Principle

> **Junior:** "I'll add a database index and increase the connection pool. That should handle the load."
>
> **Architect:** "Database optimizations help, but can't solve the fundamental problem â€” 10K writes/second will always overwhelm a single row. We need to transform 10K writes into 10 writes through debouncing, batching, and eventual consistency. The database is the last line of defense, not the first."

The key insight: **Reduce the problem before it reaches the database.** Every layer should transform high-velocity events into sustainable operations.

---

*â€” Sunchit Dudeja*  
*Day 17 of 50: System Design Interview Preparation Series*

---

> ğŸ¯ **Interview Edge:** When asked about high-write systems, explain the 7-layer approach: "Client debouncing, edge accumulation, Kafka buffering, async processing, counter caching, batched writes, approximate reads. Each layer reduces load 10x, so 10K writes/sec becomes 10 database transactions."

> ğŸ“¢ **Real Impact:** Instagram handles 500M+ daily active users, 95M+ photos/day, and billions of likes. Their 7-layer architecture means they can handle celebrity posts going viral without any infrastructure changes.

---

> ğŸ’¡ **Tomorrow (Day 18):** We'll explore **Rate Limiting** â€” how do you protect your APIs from being overwhelmed? Token Bucket, Leaky Bucket, Sliding Window, and how companies like Stripe and GitHub implement it.

